{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDN-R0V7GFw"
      },
      "source": [
        "Based on [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec?hl=en#prepare_training_data_for_word2vec).\n",
        "\n",
        "To adapt w2v to book recomendation the books replace the words. List of rated books by an user is equivalent to a \"sentence\". The book vectorization is already made by the database.\n",
        "\n",
        "Current issues :\n",
        "\n",
        "*   try to fit the book data in this implementation ==> only keep a fix amount of book for each other (10 books)\n",
        "  * need to make more lists for each user in order to have all the book represented??\n",
        "Since every list is independent, need to make book appears several time for different list of each user with different books.\n",
        "  * Increase the size of each list to fit the longest user list and fill the smaller ones with 0?\n",
        "*   Not sure what the *vocab_size* is supposed to be, the number of unique books in the data set is slightly too small.\n",
        "* Only use one dataset, may switch to others \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZSQliF1XbM"
      },
      "source": [
        "import io\n",
        "import itertools # This module implements a number of iterator building blocks inspired by constructs from APL, Haskell (brrr), and SML\n",
        "import os\n",
        "import re #regex\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import tqdm # make loops show a smart progress meter: 76%|████████████████████████       | 7568/10000 [00:33<00:10, 229.00it/s]\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "SEED = 42 \n",
        "# num_ns = 4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "c6JQvhHOxC_q",
        "outputId": "0b1ce809-cb62-4cdb-fc3d-78c4103c7121"
      },
      "source": [
        "rated = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/ratings.csv\", usecols = [\"user_id\",\"book_id\"])\n",
        "rated = rated.sort_values('user_id')\n",
        "\n",
        "rated[\"book_id\"].nunique()\n",
        "books = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/books.csv\", usecols =['book_id', 'original_title'])\n",
        "#rated = rated.set_index('book_id').join(books.set_index('book_id'))\n",
        "rated.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>book_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999485</th>\n",
              "      <td>1</td>\n",
              "      <td>140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999486</th>\n",
              "      <td>1</td>\n",
              "      <td>869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999487</th>\n",
              "      <td>1</td>\n",
              "      <td>2679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999488</th>\n",
              "      <td>1</td>\n",
              "      <td>1310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        user_id  book_id\n",
              "0             1      258\n",
              "999485        1      140\n",
              "999486        1      869\n",
              "999487        1     2679\n",
              "999488        1     1310"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JYqjbM80mu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0824ee42-4d7d-4f29-fd16-7ee4d18a985a"
      },
      "source": [
        "user_ratedbooks_ds = rated.groupby('user_id')['book_id'].apply(list).reset_index(name='book_list')\n",
        "user_ratedbooks_ds = user_ratedbooks_ds.drop('user_id', 1)\n",
        "user_ratedbooks_ds['book_list'].to_numpy()\n",
        "\n",
        "user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: np.array(x))\n",
        "user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: x[[0,1,2,3,4,5,6,7,8,9]])\n",
        "# Bad way to only have 10 book for each list. Cut the \"excess\"\n",
        "  # ==> Need to improve making of lists\n",
        "# Since this method work I susect there is no list with less than 10 items.\n",
        "  # ==> Need to implement a check of min size\n",
        "print(user_ratedbooks_ds.head())\n",
        "print(type(user_ratedbooks_ds['book_list'][0]))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                   book_list\n",
            "0      [258, 140, 869, 2679, 1310, 414, 2738, 47, 867, 1796]\n",
            "1               [131, 125, 630, 932, 8, 5, 264, 54, 66, 284]\n",
            "2  [427, 5395, 3311, 4057, 1714, 118, 1005, 1928, 7401, 484]\n",
            "3           [288, 495, 476, 325, 113, 45, 297, 11, 245, 301]\n",
            "4  [1795, 305, 6014, 887, 326, 3037, 1742, 1339, 2592, 1145]\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bEiIGpOr1zO"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "004I5ngCJmHT",
        "outputId": "d614635e-a3f4-43e3-f40f-bfe814e27f65"
      },
      "source": [
        "# I am not sure what the vocab size is supposed to be,\n",
        "# the number of unique book is slightly too small here\n",
        "vocab_size = int(rated[\"book_id\"].nunique()*1.2) \n",
        "vocab      = books[\"original_title\"].tolist()\n",
        "type(books[\"original_title\"].tolist())"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kt-3BXY1D4sZ",
        "outputId": "6397048f-c713-443a-8206-5719f39dc761"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences   = user_ratedbooks_ds['book_list'], \n",
        "    window_size = 2, \n",
        "    num_ns      = 4, \n",
        "    vocab_size  = vocab_size, \n",
        "    seed        = SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 53424/53424 [01:01<00:00, 871.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "613461 613461 613461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHWaeIutD9ZQ",
        "outputId": "a8f967b1-a36c-4f1e-f2c0-231c5bf31a11"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<BatchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n",
            "<PrefetchDataset shapes: (((1024,), (1024, 5, 1)), (1024, 5)), types: ((tf.int32, tf.int64), tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtXOD-6I0zX"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpgMoJjaI5jV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3205651b-6701-4ab5-9bdb-5c2b993c0019"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "   return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "\n",
        "\n",
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=15, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "596/596 [==============================] - 23s 37ms/step - loss: 1.5320 - accuracy: 0.3016\n",
            "Epoch 2/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 1.2694 - accuracy: 0.4334\n",
            "Epoch 3/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 1.1713 - accuracy: 0.5152\n",
            "Epoch 4/15\n",
            "596/596 [==============================] - 22s 36ms/step - loss: 1.0234 - accuracy: 0.6212\n",
            "Epoch 5/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.8853 - accuracy: 0.6918\n",
            "Epoch 6/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.7633 - accuracy: 0.7475\n",
            "Epoch 7/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.6527 - accuracy: 0.7982\n",
            "Epoch 8/15\n",
            "596/596 [==============================] - 21s 36ms/step - loss: 0.5536 - accuracy: 0.8424\n",
            "Epoch 9/15\n",
            "596/596 [==============================] - 21s 36ms/step - loss: 0.4673 - accuracy: 0.8780\n",
            "Epoch 10/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.3936 - accuracy: 0.9070\n",
            "Epoch 11/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.3315 - accuracy: 0.9296\n",
            "Epoch 12/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.2794 - accuracy: 0.9469\n",
            "Epoch 13/15\n",
            "596/596 [==============================] - 23s 38ms/step - loss: 0.2359 - accuracy: 0.9594\n",
            "Epoch 14/15\n",
            "596/596 [==============================] - 22s 37ms/step - loss: 0.1995 - accuracy: 0.9686\n",
            "Epoch 15/15\n",
            "596/596 [==============================] - 23s 38ms/step - loss: 0.1691 - accuracy: 0.9757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f44f23f7250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBCxEK1SMsZd"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43AmcxYNsAT"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = books[\"original_title\"].tolist()\n",
        "\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if  index == 0: continue # skip 0, it's padding.\n",
        "  vec = weights[index] \n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(str(word) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5AKptp8N_Ap"
      },
      "source": [
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcMSyd40bEmy"
      },
      "source": [
        "<\n",
        "\n",
        "< `vectors.tsv` & `metadata.tsv` in the notebook repo\n",
        "\n",
        "< \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4EiHHiu9SUD"
      },
      "source": [
        "---\n",
        "text example\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBu97C3HqDEB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43052d4a-d07c-46c5-b575-6361ef02d473"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "970v2W3SqIT4",
        "outputId": "c9b7fb45-5313-4607-9861-19bcdb9bf70f"
      },
      "source": [
        "with open(path_to_file) as f: \n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)\n",
        "\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BarrBuDzqpFh"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofk6_Ec7qxXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00bf9be5-f0e3-4689-f687-da7bb336d6f0"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))\n",
        "type(vectorize_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.layers.preprocessing.text_vectorization.TextVectorization"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7vXlcPGq58j",
        "outputId": "2cda25cb-c325-49d9-d8d0-817979d61ced"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YK0vTwvrAfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60178f19-9b69-4734-f6aa-3153e4bf2246"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "text_vector_ds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_UnbatchDataset shapes: (10,), types: tf.int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aZQ9Z_IrcSL",
        "outputId": "1c746058-285c-462f-d43b-85b4c5d58ee9"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))\n",
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
        "  print(type (seq))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32777\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "<class 'numpy.ndarray'>\n",
            "[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n",
            "<class 'numpy.ndarray'>\n",
            "[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n",
            "<class 'numpy.ndarray'>\n",
            "[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n",
            "<class 'numpy.ndarray'>\n",
            "[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZnga19TDnTl"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I5Fcov9rsMO"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdsbvB4FseoN"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcSxFW1nshyU"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGhnwp4hs2uN"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "embedding_dim = 128\n",
        "\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND5TDBoAtNoW"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "%tensorboard --logdir logs\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
