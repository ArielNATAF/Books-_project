{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yDN-R0V7GFw"
      },
      "source": [
        "Based on [Tensorflow tutorial](https://www.tensorflow.org/tutorials/text/word2vec?hl=en#prepare_training_data_for_word2vec).\n",
        "\n",
        "To adapt w2v to book recomendation the books replace the words. List of rated books by an user is equivalent to a \"sentence\". The book vectorization is already made by the database.\n",
        "\n",
        "Current issues :\n",
        "\n",
        "*   try to fit the book data in this implementation ==> only keep a fix amount of book for each other (10 books)\n",
        "  * need to make more lists for each user in order to have all the book represented??\n",
        "Since every list is independent, need to make book appears several time for different list of each user with different books.\n",
        "  * Increase the size of each list to fit the longest user list and fill the smaller ones with 0?\n",
        "* book selection includes every rating of user. Need to remove books from bad ratings.\n",
        "*   Not sure what the *vocab_size* is supposed to be, the number of unique books in the data set is slightly too small.\n",
        "* Only use one dataset, will switch to other \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D72-iBARfWf_"
      },
      "source": [
        "---\n",
        "# Books\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXj3gjK5feaK"
      },
      "source": [
        "## W2V"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3ZSQliF1XbM"
      },
      "source": [
        "import io\n",
        "import itertools # This module implements a number of iterator building blocks inspired by constructs from APL, Haskell (brrr), and SML\n",
        "import os\n",
        "import re #regex\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "import tqdm # make loops show a smart progress meter: 76%|████████████████████████       | 7568/10000 [00:33<00:10, 229.00it/s]\n",
        "            # cute\n",
        "import tqdm.notebook as tq # better for notebook\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Sequential\n",
        "from tensorflow.keras.layers import Activation, Dense, Dot, Embedding, Flatten, GlobalAveragePooling1D, Reshape\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE # need to read about tf Pipeline, and Prefetching\n",
        "SEED = 42 \n",
        "num_ns = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZtzs1l2dTBD"
      },
      "source": [
        "**num_ns**: negative sampled context words that do not appear in the window size neighborhood of target_word\n",
        "\n",
        "A skip-gram model predicts the context (or neighbors) of a word, given the word itself. \n",
        "The training objective of the skip-gram model is to maximize the probability of predicting context words given the target word\n",
        "\n",
        "The Noise Contrastive Estimation loss function is an efficient approximation for a full softmax. With an objective to learn word embeddings instead of modelling the word distribution, NCE loss can be simplified to use negative sampling.\n",
        "\n",
        "The simplified negative sampling objective for a target word is to distinguish the context word from num_ns negative samples drawn from noise distribution of words. More precisely, an efficient approximation of full softmax over the vocabulary is, for a skip-gram pair, to pose the loss for a target word as a classification problem between the context word and num_ns negative samples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6JQvhHOxC_q"
      },
      "source": [
        "\n",
        "rated = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/ratings.csv\", usecols = [\"user_id\",\"book_id\"])\n",
        "rated = rated.sort_values('user_id')\n",
        "\n",
        "rated[\"book_id\"].nunique()\n",
        "books = pd.read_csv(\"https://raw.githubusercontent.com/ArielNATAF/Books_project/dev/datasets/books.csv\", usecols =['book_id', 'original_title'])\n",
        "rated.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl2gPCD-hpkN"
      },
      "source": [
        "# Split lists of ds in list of 10:\n",
        "#   • Select a list in the ds (a list: a row in book_list column)\n",
        "#   • Check size lists of the ds\n",
        "#   • If not multiple of 10 itemss in list\n",
        "#       • Pad with 0 to multiple of 10 items\n",
        "#   • If more than 10 items in list\n",
        "#       • Copy last 10 items of list in new list\n",
        "#       • Remove last 10 items of current list\n",
        "#       • Add the new list to the ds\n",
        "#   • Go to next list\n",
        "\n",
        "# Need to be vastly optimized.\n",
        "#   • Create a  temp Series with number of cell to add,\n",
        "#   • Add the lists in each cells pf temp\n",
        "#   • append the filled temp after the loop\n",
        "# (should divide by 10 the number of appends)\n",
        "\n",
        "# Currently only a small part of dataset to save the RAM\n",
        "import timeit\n",
        "\n",
        "def split_list(ds):\n",
        "\n",
        "  def pad_list(lst):\n",
        "    if not lst.size % 10 == 0:\n",
        "      lst = np.pad(lst, (0, 10 - (lst.size % 10)), mode='constant', constant_values=0)\n",
        "    return lst\n",
        "\n",
        "  print('count at start:',ds.count())\n",
        "  # count at start: 53424\n",
        "  start = timeit.default_timer()\n",
        "\n",
        "  print(ds[0].size,ds[1].size,ds[2].size,ds[3].size,ds[4].size)\n",
        "  ds = ds.apply(lambda x: pad_list(x))\n",
        "  print(ds[0].size,ds[1].size,ds[2].size,ds[3].size,ds[4].size)\n",
        "\n",
        "  for i in tq.tqdm(range(3000)):\n",
        "\t  while ds[i].size > 10:\n",
        "\t    size = ds[i].size\n",
        "\t    indice=[size-10,size-9,size-8,size-7,size-6,size-5,size-4,size-3,size-2,size-1]\n",
        "\t    new_list=ds[i][indice]\n",
        "\t    ds[i] = np.delete(ds[i],indice,0)\n",
        "\t    ds = ds.append(pd.Series([new_list]), ignore_index=True)\n",
        "\n",
        "  print('count at end:',ds.count())\n",
        "  # count at end: 621633\n",
        "    \n",
        "  stop = timeit.default_timer()\n",
        "  print('Time: ', int(stop - start)) # Time:  3200 => 53mn\n",
        "  return ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JYqjbM80mu_"
      },
      "source": [
        "#Group the books of each user in lists in one column\n",
        "user_ratedbooks_ds = rated.groupby('user_id')['book_id'].apply(list).reset_index(name='book_list')\n",
        "user_ratedbooks_ds = user_ratedbooks_ds.drop('user_id', 1)\n",
        "user_ratedbooks_ds['book_list'].to_numpy()\n",
        "\n",
        "user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: np.array(x))\n",
        "user_ratedbooks_ds['book_list'] = split_list(user_ratedbooks_ds['book_list'])\n",
        "# Too big, will eat most of RAM memory, so only affect first few thousands of row\n",
        "user_ratedbooks_ds['book_list'] = user_ratedbooks_ds['book_list'].apply(lambda x: x[[0,1,2,3,4,5,6,7,8,9]])\n",
        "# Bad way to only have 10 book for each list. Cut the \"excess\"\n",
        "  # ==> Need to improve making of lists\n",
        "# No user has less than 10 books rated (min=19) currently.\n",
        "  # function to check min/max sizes in ##Misc section\n",
        "\n",
        "print(user_ratedbooks_ds.head())\n",
        "print(type(user_ratedbooks_ds['book_list'][0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bEiIGpOr1zO"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tq.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "004I5ngCJmHT"
      },
      "source": [
        "# I am not sure what the vocab size is supposed to be,\n",
        "# the number of unique book is slightly too small here\n",
        "vocab_size = int(rated[\"book_id\"].nunique()*1.2) \n",
        "vocab      = books[\"original_title\"].tolist()\n",
        "type(books[\"original_title\"].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt-3BXY1D4sZ"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences   = user_ratedbooks_ds['book_list'], \n",
        "    window_size = 2, \n",
        "    num_ns      = 4, \n",
        "    vocab_size  = vocab_size, \n",
        "    seed        = SEED)\n",
        "print(len(targets), len(contexts), len(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWaeIutD9ZQ"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXtXOD-6I0zX"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpgMoJjaI5jV"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "   return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "\n",
        "\n",
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=15, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBCxEK1SMsZd"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43AmcxYNsAT"
      },
      "source": [
        "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
        "vocab = books[\"original_title\"].tolist()\n",
        "\n",
        "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for index, word in enumerate(vocab):\n",
        "  if  index == 0: continue # skip 0, it's padding.\n",
        "  vec = weights[index] \n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "  out_m.write(str(word) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcMSyd40bEmy"
      },
      "source": [
        "<\n",
        "\n",
        "< `vectors.tsv` & `metadata.tsv` in the notebook repo\n",
        "\n",
        "< \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5AKptp8N_Ap"
      },
      "source": [
        "#download the files\n",
        "try:\n",
        "  from google.colab import files\n",
        "  files.download('vectors.tsv')\n",
        "  files.download('metadata.tsv')\n",
        "except Exception as e:\n",
        "  pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIEnkPdvfnbh"
      },
      "source": [
        "## Misc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFBiYgaWeIIo"
      },
      "source": [
        "def check_min(ds=user_ratedbooks_ds['book_list']):\n",
        "  ds = ds.apply(lambda x: x.size)\n",
        "  print(\"Min size:\", ds.min())\n",
        "  print(\"Max size:\", ds.max())\n",
        "\n",
        "check_min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qbKOpPEgGB9"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4EiHHiu9SUD"
      },
      "source": [
        "---\n",
        "# TextExample\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBu97C3HqDEB"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "970v2W3SqIT4"
      },
      "source": [
        "with open(path_to_file) as f: \n",
        "  lines = f.read().splitlines()\n",
        "for line in lines[:20]:\n",
        "  print(line)\n",
        "\n",
        "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BarrBuDzqpFh"
      },
      "source": [
        "# We create a custom standardization function to lowercase the text and \n",
        "# remove punctuation.\n",
        "def custom_standardization(input_data):\n",
        "  lowercase = tf.strings.lower(input_data)\n",
        "  return tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation), '')\n",
        "\n",
        "# Define the vocabulary size and number of words in a sequence.\n",
        "vocab_size = 4096\n",
        "sequence_length = 10\n",
        "\n",
        "# Use the text vectorization layer to normalize, split, and map strings to\n",
        "# integers. Set output_sequence_length length to pad all samples to same length.\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=custom_standardization,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofk6_Ec7qxXR"
      },
      "source": [
        "vectorize_layer.adapt(text_ds.batch(1024))\n",
        "type(vectorize_layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vXlcPGq58j"
      },
      "source": [
        "# Save the created vocabulary for reference.\n",
        "inverse_vocab = vectorize_layer.get_vocabulary()\n",
        "print(inverse_vocab[:20])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YK0vTwvrAfT"
      },
      "source": [
        "def vectorize_text(text):\n",
        "  text = tf.expand_dims(text, -1)\n",
        "  return tf.squeeze(vectorize_layer(text))\n",
        "\n",
        "# Vectorize the data in text_ds.\n",
        "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
        "text_vector_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aZQ9Z_IrcSL"
      },
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))\n",
        "for seq in sequences[:5]:\n",
        "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n",
        "  print(type (seq))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZnga19TDnTl"
      },
      "source": [
        "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
        "# (int-encoded sentences) based on window size, number of negative samples\n",
        "# and vocabulary size.\n",
        "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
        "  # Elements of each training example are appended to these lists.\n",
        "  targets, contexts, labels = [], [], []\n",
        "\n",
        "  # Build the sampling table for vocab_size tokens.\n",
        "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
        "\n",
        "  # Iterate over all sequences (sentences) in dataset.\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "\n",
        "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
        "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "          sequence, \n",
        "          vocabulary_size=vocab_size,\n",
        "          sampling_table=sampling_table,\n",
        "          window_size=window_size,\n",
        "          negative_samples=0)\n",
        "\n",
        "    # Iterate over each positive skip-gram pair to produce training examples \n",
        "    # with positive context word and negative samples.\n",
        "    for target_word, context_word in positive_skip_grams:\n",
        "      context_class = tf.expand_dims(\n",
        "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_class,\n",
        "          num_true=1, \n",
        "          num_sampled=num_ns, \n",
        "          unique=True, \n",
        "          range_max=vocab_size, \n",
        "          seed=SEED, \n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "      # Build context and label vectors (for one target word)\n",
        "      negative_sampling_candidates = tf.expand_dims(\n",
        "          negative_sampling_candidates, 1)\n",
        "\n",
        "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
        "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
        "\n",
        "      # Append each element from the training example to global lists.\n",
        "      targets.append(target_word)\n",
        "      contexts.append(context)\n",
        "      labels.append(label)\n",
        "\n",
        "  return targets, contexts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I5Fcov9rsMO"
      },
      "source": [
        "targets, contexts, labels = generate_training_data(\n",
        "    sequences=sequences, \n",
        "    window_size=2, \n",
        "    num_ns=4, \n",
        "    vocab_size=vocab_size, \n",
        "    seed=SEED)\n",
        "print(len(targets), len(contexts), len(labels))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdsbvB4FseoN"
      },
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)\n",
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcSxFW1nshyU"
      },
      "source": [
        "class Word2Vec(Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = Embedding(vocab_size, \n",
        "                                      embedding_dim,\n",
        "                                      input_length=1,\n",
        "                                      name=\"w2v_embedding\", )\n",
        "    self.context_embedding = Embedding(vocab_size, \n",
        "                                       embedding_dim, \n",
        "                                       input_length=num_ns+1)\n",
        "    self.dots = Dot(axes=(3,2))\n",
        "    self.flatten = Flatten()\n",
        "\n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    we = self.target_embedding(target)\n",
        "    ce = self.context_embedding(context)\n",
        "    dots = self.dots([ce, we])\n",
        "    return self.flatten(dots)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGhnwp4hs2uN"
      },
      "source": [
        "def custom_loss(x_logit, y_true):\n",
        "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n",
        "embedding_dim = 128\n",
        "\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND5TDBoAtNoW"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "%tensorboard --logdir logs\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
